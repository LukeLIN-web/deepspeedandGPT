演讲稿

李航老师在开营talk里提到了GPT大模型，但大模型不是想训就能训的，存在一系列系统挑战。 因此我们的项目就是研究这些挑战。 

我们的展示分为4部分，第1部分是总体介绍，第2部分是优化方法，第3部分是实验结果，然后第4部分是结论和展望

 深度学习模型大小的增加提供了显著的准确度提升。 在自然语言处理领域,transformer 为大型模型铺平了道路，出现了例如 Bert-large (0.3亿参数) 、GPT-2 (15亿参数) 、Megatron-LM (83亿参数) ，T5 (110亿参数) 等大型模型, 随着模型规模从数千万个参数持续增长到数千亿个参数, 我们面临着挑战.

 (切ppt)

模型占据的空间显然超出单个设备（例如 GPU 或 TPU）的内存，添加更多设备无法再扩大训练规模。因此, 我们需要拆分模型到各个设备上.  利用模型并行将每层中的参数划分到多个设备，需要每层之间进行大量通信。

我们的工作是探索在单机八GPU上能容纳最大模型的极限. 我们的工作可以让没有太多GPU的算法工程师也可以微调大模型.  不同于耗费巨大资源和时间的预训练, 模型的微调可以较快地让巨大的模型应用到其他任务上. 

 (切ppt)

我们在单机上成功训练 150 亿参数模型，是 baseline 的 20 倍, Baseline采用最常见的 PyTorch 多卡数据并行. 我们可以看一下右边这张图 , 蓝色的柱子是pytorch baseline, 绿色的柱子是我们的工作.

下面由我队友讲解优化方法.



 优化方法有五点, 包括梯度累积，混合精度，内存显存交换，重算和参数/梯度/优化器状态切分。下面我们将一一介绍：

1.  梯度累积是指不一次性求整个批次样本的梯度, 而是先算一部分样本，得到梯度，再导入剩余样本重复计算若干次，将得到的梯度累积到最后一起更新。比如一个batch=8的数据，8*16的矩阵输入，乘上一个 16*16的参数矩阵，最后需要额外8*16的矩阵来储存结果。如果我们使用梯度累积的方法，每次使用batch=1,1*16的矩阵输入，这样就只需要1*16的额外空间来储存结果了。
2.  混合精度：混合精度优化是指将原来的32位存储浮点数改成16位存储和32位混合存储和计算, 可以减少一半的存储空间并提升计算速度，其中底层tensor还有针对16位浮点数的优化。比如softmax要求的精度比较高，我们会使用32位来计算。而在gradient的计算中我们一般采用16位精度，计算完成后把它存到32位精度parameter里面。
3. 内存显存交换是指把GPU上没用到的存储模型状态量暂时放到CPU内存上，等到需要时再放到GPU上运算。在计算gradient的时候，由于GPU显存不足，我们会把GPU计算得到的结果统一换到CPU来做更新，更新完以后再把参数换到各个GPU。
4. 重算是指将庞大的中间结果丢弃，保存一些特定的状态点。 当反向过程中需要用到中间结果时，从最近的状态点重新开始计算，利用时间省下GPU的空间。比如一层网络是由卷积、relu和pooling组成的，我们会把checkpoint打到卷积的位置，因为relu和pooling需要的额外空间比较大但是计算复杂度不高。这样我们在反向传播需要用到relu和pooling这样的中间量的时候，就从卷积开始算，这样能有效降低显存使用量。我们在进行混合精度训练时，存储模型状态量（参数、梯度和优化器状态量）就需要约 160GB 的显存。而激活函数额外消耗的显存在batch 设置为1的情况下，训练百亿参数模型就会产生超过10GB 的激活函数用的显存。用 checkpoint 处理激活显存，用计算来换显存，可以将需要的显存减少到大约200MB。
5. 切分是指将一个模型的参数、梯度和优化器状态切分到各个GPU上以容纳更大的模型。一般来说，zero有三个stage的优化，stage1切分了优化器状态量（保存方差， 动量），stage2切分了gradient，stage3进一步切分了参数。当达到 stage3的时候，我们参数更新就会换到cpu上面更新，计算完成后重新分发到各个GPU。该方法本质上是充分利用计算时间进行通讯，达到计算时间和通讯时间的一个平衡。





下面我们展示我们的实验成果



首先我们给出我们的实验配置，我们的实验主要运行在一台有8块16GB显存的v100和330GB内存的机器上，实验基于PyTorch实现，版本为1.9.0。

接着这张图中展示了几种配置下的训练模型大小及训练速度，横轴表示模型参数大小，单位为百万个，纵轴为平均训练速度，单位为Samples Per Sec。

蓝色这条线展示了在启用以上五点优化后能训练的模型参数大小，以及对应的训练速度，可以看到在启用所有优化后已经可以实现50亿参数的训练，是baseline的7倍以上。

图中的其他线代表在关闭对应的优化技术后得到的训练结果。橙色线可以看出梯度累积技术，虽然对训练速度有一定影响，但是使得模型训练大小由29亿提升至50亿；绿色的线可以看出offload技术对提升模型训练大小效果明显，在不应用offload技术的情况下仅能训到9亿参数左右,开启后可以达到50亿参数；checkpoint技术在略微降低训练速度的情况下，提升模型训练大小；可以看到在不应用混合精度优化后，其训练模型大小极限降低，且训练速度也有一些下降，但是训练精度会提升.

最后我们给出在单机八卡上的极限数据，我们采用了以上5点技术的基础上，梯度累积的划分的step由2提到8，并且在cpu上也启用了checkpoint技术。我们首先估算了一下在单机上的参数训练大小极限，左下角的表格给出了单个参数的内存占用（约20B），主要包括fp16精度的梯度和参数，fp32精度的梯度和参数，以及fp32精度的优化器状态（由于我们使用的是Adam优化器，所以包括momentum和variance），以内存为300G为基础计算，我们可以得到，在我们的机器上可以实现150亿参数的训练。



pytorch 自带了混合精度。 其他需要自己有一些实现。



结论:

 我们成功训练了150亿参数的模型, 通过内存显存交换（offload）把GPU上没用到的中间结果暂时放到CPU内存上，等到需要时再放到GPU上运算。  



我们的在参数量巨大的时候CPU内存无法容纳所有参数以及中间结果,   这也就是目前性能瓶颈, 一种解决方法是加大内存, 第二种方法是将CPU内存offload到磁盘中.



我们未来的工作有两点 第一点是 如果我们可以把中间结果swap 到容量更大的NVME磁盘，因为磁盘的空间比内存大几十倍, 我们就可以训练更大的模型. 

第二点是 跨机器进行参数分割,  跨GPU的机内8卡我们已经实现了, 下一步尝试 在分布式机器上进行参数分割来训练超大模型.

(这里要考虑网卡的传输速度更慢, 调优将面临更多挑战. 哪些挑战? 隐藏传输时间, 让传输先于计算完成)



以上就是我们的展示, 谢谢!





项目的难度在哪里?

1. 如何找出目前深度学习框架pytorch训练大模型的瓶颈。
2. 如何使用各种显存优化技术进行极致优化

突破点：

1. 找出了目前深度学习框架pytorch训练大模型的瓶颈。
2. 量化了各显存优化技术对训练大模型的贡献度
3. 找出了单机训练模型参数量的极限（150亿参数， baseline的20倍）



关于如何找出PyTorch框架的大模型训练瓶颈，就是使用pytorch profiler看显存使用情况，并了解显存使用的构成（例如parameters/gradients/optimizer states/activations）和可优化的空间。然后再加一个优化技术，再看看系统瓶颈在哪里，一步步朝着大模型极限方向探索









项目的意义： 

可以训练巨大模型， 实现更多任务。

业界有哪些代表性公司在做

1. 华为云发布包括30亿参数的全球最大视觉（CV）预训练模型，以及盘古千亿参数的全球最大中文语言（NLP）预训练模型。分类、阅读理解单项均排名第一
2. 2021大会，来自北大、清华、中科院的100余位AI科学家团队悟道2.0 ，是一个全能选手，一统文本和视觉两大领域，在问答、绘画、作诗、视频等任务中正在逼近图灵测试。
3. 谷歌大脑最新论文Switch Transformer拥有超过1.6万亿的参数，是迄今为止规模最大的NLP模型。
4. openai做了很多transformer的相关大模型。

以及再了解下GPT模型

是由旧金山人工智能研究实验室 OpenAI 创建的语言模型，可以在接近人类的水平上执行阅读理解和写作任务，只是它看到的文本比任何人一生阅读的文本都多。它可以执行各种各样的自然语言任务并生成类似人类的文本， 翻译， 文本生成， 文本分类，各种任务都可以。



[What is GPT-3 and why is it so powerful? | Towards Data Science](https://towardsdatascience.com/what-is-gpt-3-and-why-is-it-so-powerful-21ea1ba59811)

GPT-3（Generative Pre-trained Transformer 3）是由旧金山人工智能研究实验室 OpenAI 创建的语言模型。  1750 亿参数的深度学习模型能够生成类似人类的文本，并在数千亿字的大型文本数据集上进行了训练。
“我对拥有 302 个神经元的蠕虫有意识的想法持开放态度，因此我对拥有 1750 亿个参数的 GPT-3 也有意识的想法持开放态度。”  — 大卫·查默斯

自去年夏天以来，GPT-3 成为头条新闻，整个初创公司都使用此工具创建。 然而，重要的是要了解 GPT-3 真正是什么以及它是如何工作的背后的事实，而不是迷失在围绕它的所有炒作中，并将其视为可以解。决任何问题的黑匣子。
在本文中，我将向您简要介绍 GPT-3 的工作原理，以及该模型的优势和局限性以及您如何自己使用它。

GPT-3 的工作原理 是什么？

GPT-3 的核心是一个变压器模型。  Transformer 模型是序列到序列的深度学习模型，可以在给定输入序列的情况下生成文本序列。 这些模型专为文本生成任务而设计，例如问答、文本摘要和机器翻译。 下图展示了在给定英语输入序列的情况下，transformer 模型如何迭代生成法语翻译。

Transformer 模型的运行方式与 LSTM 不同，它使用多个称为注意力块的单元来了解文本序列的哪些部分需要重点关注。 单个 Transformer 可能有几个单独的注意力块，每个块都学习语言的不同方面，从词性到命名实体。 要深入了解变压器的工作原理，您应该查看我下面的文章。

GPT-3 是 OpenAI 创建的第三代 GPT 语言模型。  GPT-3 与之前型号的主要区别在于其尺寸。  GPT-3 包含 1750 亿个参数，是 GPT-2 的 17 倍，是微软图灵 NLG 模型的 10 倍左右。 参考我上面列出的上一篇文章中描述的转换器架构，GPT-3 有 96 个注意力块，每个块包含 96 个注意力头。 换句话说，GPT-3 基本上是一个巨型变压器模型。
基于介绍该模型的原始论文，GPT-3 使用以下大型文本数据集的组合进行训练：

- Common Crawl WebText2 Books1 Books2 Wikipedia Corpus

 最终数据集包含来自互联网的大部分网页、大量书籍以及维基百科的全部内容。 研究人员使用这个包含数千亿单词的数据集来训练 GPT-3 以生成其他几种语言的英语文本。

为什么 GPT-3 好用？ 

 GPT-3 自去年夏天以来就成为头条新闻，因为它可以执行各种各样的自然语言任务并生成类似人类的文本。  GPT-3 可以执行的任务包括但不限于： 文本分类（即情感分析） 问答 文本生成 文本摘要 命名实体识别 语言翻译

基于 GPT-3 可以执行的任务，我们可以将其视为一个模型，可以在接近人类的水平上执行阅读理解和写作任务，只是它看到的文本比任何人一生阅读的文本都多。 这正是 GPT-3 如此强大的原因。 整个初创公司都是用 GPT-3 创建的，因为我们可以将其视为通用的瑞士军刀，用于解决自然语言处理中的各种问题。

GPT-3 的局限性是什么？ 

 虽然在撰写本文时 GPT-3 是最大且可以说是最强大的语言模型，但它也有其自身的局限性。 事实上，每一个机器学习模型，无论多么强大，都有一定的局限性。 这个概念是我在下面关于无免费午餐定理的文章中详细探讨的。

考虑下面列出的 GPT-3 的一些局限性： GPT-3 缺乏长期记忆——该模型不会像人类一样从长期交互中学习任何东西。
缺乏可解释性——这是一个影响极其庞大和复杂的问题。  GPT-3 太大了，很难解释或解释它产生的输出。
有限的输入大小——转换器有一个固定的最大输入大小，这意味着 GPT-3 可以处理的提示不能超过几句话。
推理时间慢——因为 GPT-3 太大，模型需要更多时间来产生预测。
GPT-3 存在偏差——所有模型的好坏取决于用于训练它们的数据，GPT-3 也不例外。 例如，这篇论文证明了 GPT-3 和其他大型语言模型包含反穆斯林偏见。

尽管 GPT-3 功能强大，但它仍然存在局限性，使其远非完美的语言模型或通用人工智能 (AGI) 的示例。

如何使用 GPT-3 ？

GPT-3 不是开源的，OpenAI 决定改为通过商业 API 提供模型，您可以在此处找到。 此 API 处于内测阶段，这意味着您必须填写 OpenAI API 候补表才能加入候补名单才能使用该 API。
OpenAI 还为想要使用 GPT-3 的学术研究人员提供了一个特殊程序。 如果您想将 GPT-3 用于学术研究，您应该填写学术访问申请。

虽然 GPT-3 不是开源或公开可用的，但其前身 GPT-2 是开源的，可通过 Hugging Face 的转换器库访问。 如果您想改用这个较小但仍然强大的语言模型，请随时查看 Hugging Face 的 GPT-2 实现文档。

总结 

自去年夏天以来，GPT-3 受到了很多关注，因为它是迄今为止在撰写本文时创建的最大且可以说是最强大的语言模型。 然而，GPT-3 仍然受到一些限制，使其远非完美的语言模型或 AGI 的例子。 如果您想将 GPT-3 用于研究或商业目的，您可以申请使用 Open AI 的 API，该 API 目前处于内测阶段。 否则，由于 HuggingFace 的转换器库，您始终可以直接使用公开可用且开源的 GPT-2。







反思:

不足之处在于xxxx



我们之后还可以在xxx地方改进.





（1）主题名称

（2）团队介绍——1页 

（3）项目背景与分析思路清晰——简要 

（4）项目原理与成果——重点 

（5）结论及反思——重点

（6）其他内容可自行添加及发挥

