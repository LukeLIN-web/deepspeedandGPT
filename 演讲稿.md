演讲稿

我们的展示分为4部分，第1部分是总体介绍，第2部分是优化方法，第3部分是实验结果，然后第4部分是结论和展望

首先我们来讲一下我们项目的背景

 深度学习模型大小的增加提供了显著的准确度提升。 在自然语言处理领域,transformer 为大型模型铺平了道路，出现了例如 Bert-large (0.3亿参数) 、GPT-2 (15亿参数) 、Megatron-LM (83亿参数) ，T5 (110亿参数) 等大型模型, 随着模型规模从数千万个参数持续增长到数千亿个参数, 我们面临着挑战.

 (切ppt)

模型占据的空间显然超出单个设备（例如 GPU 或 TPU）的内存，添加更多设备无法再扩大训练规模。因此, 我们需要拆分模型到各个设备上.  利用模型并行将每层中的参数划分到多个设备，需要每层之间进行大量通信。

我们的工作是探索在单机八GPU上能容纳最大模型的极限. 我们的工作可以让没有太多GPU的算法工程师也可以微调大模型.  不同于耗费巨大资源和时间的预训练, 模型的微调可以较快地让巨大的模型应用到其他任务上. 

 (切ppt)

我们在单机上成功训练 150 亿参数模型，是 baseline 的 20 倍, Baseline采用最常见的 PyTorch 多卡数据并行. 我们可以看一下右边这张图 , 蓝色的柱子是pytorch baseline, 绿色的柱子是我们的工作.

下面由我队友讲解优化方法.



 优化方法有五点, 包括梯度累积，混合精度，内存显存交换，重算和参数/梯度/优化器状态切分。下面我们将一一介绍：

1.  梯度累积是指不一次性求整个批次样本的梯度, 而是先算一部分样本，得到梯度，再导入剩余样本重复计算若干次，将得到的梯度累积到最后一起更新。比如一个batch=8的数据，8*16的矩阵输入，乘上一个 16*16的参数矩阵，最后需要额外8*16的矩阵来储存结果。如果我们使用梯度累积的方法，每次使用batch=1,1*16的矩阵输入，这样就只需要1*16的额外空间来储存结果了。
2.  混合精度：混合精度优化是指将原来的32位存储浮点数改成16位存储和32位混合存储和计算, 可以减少一半的存储空间并提升计算速度，其中底层tensor还有针对16位浮点数的优化。比如softmax要求的精度比较高，我们会使用32位来计算。而在gradient的计算中我们一般采用16位精度，计算完成后把它存到32位精度parameter里面。
3. 内存显存交换是指把GPU上没用到的存储模型状态量暂时放到CPU内存上，等到需要时再放到GPU上运算。在计算gradient的时候，由于GPU显存不足，我们会把GPU计算得到的结果统一换到CPU来做更新，更新完以后再把参数换到各个GPU。
4. 重算是指将庞大的中间结果丢弃，保存一些特定的状态点。 当反向过程中需要用到中间结果时，从最近的状态点重新开始计算，利用时间省下GPU的空间。比如一层网络是由卷积、relu和pooling组成的，我们会把checkpoint打到卷积的位置，因为relu和pooling需要的额外空间比较大但是计算复杂度不高。这样我们在反向传播需要用到relu和pooling这样的中间量的时候，就从卷积开始算，这样能有效降低显存使用量。我们在进行混合精度训练时，存储模型状态量（参数、梯度和优化器状态量）就需要约 160GB 的显存。而激活函数额外消耗的显存在batch 设置为1的情况下，训练百亿参数模型就会产生超过10GB 的激活函数用的显存。用 checkpoint 处理激活显存，用计算来换显存，可以将需要的显存减少到大约200MB。
5. 切分是指将一个模型的参数、梯度和优化器状态切分到各个GPU上以容纳更大的模型。一般来说，zero有三个stage的优化，stage1切分了优化器状态量，stage2切分了gradient，stage3进一步切分了参数。当达到 stage3的时候，我们参数更新就会换到cpu上面更新，计算完成后重新分发到各个GPU。该方法本质上是充分利用计算时间进行通讯，达到计算时间和通讯时间的一个平衡。





下面我们展示我们的实验成果



首先我们给出我们的实验配置，我们的实验主要运行在一台有8块16GB显存的v100和330GB内存的机器上，实验基于PyTorch实现，版本为1.9.0。

接着这张图中展示了几种配置下的训练模型大小及训练速度，横轴表示模型参数大小，单位为百万个，纵轴为平均训练速度，单位为Samples Per Sec。

蓝色这条线展示了在启用以上五点优化后能训练的模型参数大小，以及对应的训练速度，可以看到在启用所有优化后已经可以实现50亿参数的训练，是baseline的7倍以上。

图中的其他线代表在关闭对应的优化技术后得到的训练结果。橙色线可以看出梯度累积技术，虽然对训练速度有一定影响，但是使得模型训练大小由29亿提升至50亿；绿色的线可以看出offload技术对提升模型训练大小效果明显，在不应用offload技术的情况下仅能训到9亿参数左右,开启后可以达到50亿参数；checkpoint技术在略微降低训练速度的情况下，提升模型训练大小；可以看到在不应用混合精度优化后，其训练模型大小极限降低，且训练速度也有一些下降，但是训练精度会提升.

最后我们给出在单机八卡上的极限数据，我们采用了以上5点技术的基础上，梯度累积的划分更细了，并且在cpu上也启用了checkpoint技术。我们首先估算了一下在单机上的参数训练大小极限，左下角的表格给出了单个参数的内存占用（约20B），也就是在300GB的内存上，可以实现150亿参数的训练。





结论:

 我们成功训练了150亿参数的模型, 通过内存显存交换（offload）把GPU上没用到的中间结果暂时放到CPU内存上，等到需要时再放到GPU上运算。  



我们的在参数量巨大的时候CPU内存无法容纳所有参数以及中间结果,   这也就是目前性能瓶颈, 一种解决方法是加大内存, 第二种方法是将CPU内存offload到磁盘中.



我们未来的工作有两点 第一点是 如果我们可以把中间结果swap 到容量更大的NVME磁盘，因为磁盘的空间比内存大几十倍, 我们就可以训练更大的模型. 

第二点是 跨机器进行参数分割,  跨GPU的机内8卡我们已经实现了, 下一步尝试 在分布式机器上进行参数分割来训练超大模型.

(这里要考虑网卡的传输速度更慢, 调优将面临更多挑战. 哪些挑战? 隐藏传输时间, 让传输先于计算完成)



以上就是我们的展示, 谢谢!







反思:

不足之处在于xxxx



我们之后还可以在xxx地方改进.





（1）主题名称

（2）团队介绍——1页 

（3）项目背景与分析思路清晰——简要 

（4）项目原理与成果——重点 

（5）结论及反思——重点

（6）其他内容可自行添加及发挥