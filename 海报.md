海报



我们考虑一个普通正向/反向神经网络模型，当模型参数数量越来越大的时候，所需空间会超出GPU的显存。所以我们需要考虑一个通用的方法来满足超出GPU显存的需求，而DeepSpeed恰恰解决了这一问题。

本项目在1个节点8个GPU上训练超过130亿个参数的GPT-2模型，与PyTorch等流行框架相比，它可以训练的大小增加了10倍，而且不需要对模型进行任何更改，也不需要牺牲计算效率。 对于10B参数模型， ZeRO-Offload可以在单个NVIDIA V100 GPU上实现40 TFlops/GPU。

  DeepSpeed在规模、速度、成本和可用性方面都非常出色。左下角的图描述了与单独使用Megatron-LM相比，DeepSpeed（将zero数据并行性与Megatron-LM的模型并行性相结合）的系统吞吐量改进。右下角的图比较了仅使用数据并行性的可训练模型大小（有zero和无zero）。

这些字母的含义？

 psi 代表参数大小，  K表示啥，   Nd表示啥？  

不同行之间的流程/逻辑关系



梯度可以存外面， 用到再拿进来？ 

parameters比较复杂。

在训练期间，我们首先通过前向传播计算损失。由于 fp16 参数已经呈现在 GPU 上，因此这部分计算不需要 CPU 通信。在损失的反向传播期间，不同参数的梯度在反向调度的不同点计算。 ZeRO-Offload 可以在计算后立即将每个参数的这些梯度单独或以group形式传输到 CPU 内存。因此，在将梯度传输到 CPU 内存之前，只需要少量内存就可以将梯度暂时保存在 GPU 内存上。此外，每个梯度转移都可以与反向图其余部分的反向传播重叠，从而允许ZeRO-Offload 降低大部分通信成本。在反向传播后，ZeRO-Offload 直接在 CPU 上更新 fp32 参数和剩余的optimizer状态（如momentum和方差），并将更新后的 fp32 参数从 CPU 内存复制到 GPU 内存上的 fp16 参数。



zeroload可以训练一万亿参数的模型，相比pytorch可以训练多10倍的参数



结论： 从HPC和系统的角度来看，Zero代表了大型模型训练领域的革命性变革。而我们的实现zero-100B 让模型大小增加8倍，吞吐量提高10倍以上，实现在现代GPU集群上超线性加速，并训练世界上最大的模型。从整体上看，ZeRO有可能将模型大小再增加一个数量级，从而能够训练未来的万亿参数模型。最乐观的是zero对数据科学家没有任何障碍，与MP和PP等现有方法不同，无需进行模型重构，而且与标准DP一样易于使用，这让ZeRO成为大型模型训练未来科研的首选对象。





参数的计算

hidden_size = h

我们是把每一个模块参数数量加起来，   q的参数数量因为是 self.attention，就是 h *h，

这个layerd总共的参数数量是12h*h， 

Hidden size = 词向量的维度